# -*- coding: utf-8 -*-
"""NetflixData_mining_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1961WfTyzucDV3-j4UN3m8kQ0vDVvKIWG
"""

w#pip install scikit-surprise

import pandas as pd
import numpy as np
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from surprise import Reader, Dataset, SVD
from surprise.model_selection import cross_validate

"""#Importing the dataset 
1. The dataset is given in a online hackathon by NETFLIX asking participants to Suggest the New movies for users based on their previous ratings given by all users. 
2. all the datasets are imported one by one and they are combined to make the final complete dataset.
"""

df1 = pd.read_csv('/content/drive/MyDrive/Data_Mining_Project/combined_data_1.txt', header = None, names = ['Customer_Id', 'Rating'])
df2 = pd.read_csv('/content/drive/MyDrive/Data_Mining_Project/combined_data_2.txt', header = None, names = ['Customer_Id', 'Rating'])
df3 = pd.read_csv('/content/drive/MyDrive/Data_Mining_Project/combined_data_3.txt', header = None, names = ['Customer_Id', 'Rating'])       #importing thr datasets from drive
df4 = pd.read_csv('/content/drive/MyDrive/Data_Mining_Project/combined_data_4.txt', header = None, names = ['Customer_Id', 'Rating'])



df1['Rating'] = df1['Rating'].astype(float)
df2['Rating'] = df2['Rating'].astype(float)
df3['Rating'] = df3['Rating'].astype(float)                   #converting the rating variable into float type.
df4['Rating'] = df4['Rating'].astype(float)


d = df1
d = d.append(df1)
d = d.append(df2)                                             # appending the datasets into one
d = d.append(df3)
d = d.append(df4)

"""# data view
1. as we can see the data is organised in a way that first comes the movie id and then comes the raing given by all users to that movie
2. There are four datasets given that contain rating given by users for each movie 
3. Dimensions  97,704,614 X 2 => 97.7 Million user ratings . A Total of l 17,434 movies listed. Need to extract features directly from patterns in data
"""

print(d.shape)
d.head()

df.index = np.arange(0,len(df))
print('Full dataset shape: {}'.format(df.shape))
print(df.iloc[::, :])
df.head()

"""## Plotting the user ratings
1. the below graph gives the approximation about the number of users and movies and also the number of ratings given by them

2. The are a total of 474,954 Customers A total of 17,866 Movies 97 Million Customer ratings given 
3. The ratings are a bit Positively biased as only satisfied customers keep using the platform Mostly the people who are unsatisfied prefer leaving the Platform rather than rating for bad movies.
"""

p = df.groupby('Rating')['Rating'].agg(['count'])

# get movie and customer count count
total_movies    = df.isnull().sum()[1]
total_customers = df['Customer_Id'].nunique() - total_movies
total_ratings   = df['Customer_Id'].count()   - total_movies

ax = p.plot(kind = 'barh', legend = False, figsize = (15,10))
plt.title('Total pool: {:,} Movies, {:,} customers, {:,} ratings given'.format(total_movies, total_customers, total_ratings ), fontsize=20)
plt.axis('off')

for i in range(1,6):
    ax.text(p.iloc[i-1][0]/4, i-1, 'Rating {}: {:.0f}%'.format(i, p.iloc[i-1][0]*100 / p.sum()[0]), color = 'white', weight = 'bold')

#taking the movie_id from the df and appending as a column in each iteration
#to get the movie_id we look for Nan values in the ratings column and use that as a seperator between one movie ratinf and the other
df_nan = pd.DataFrame(pd.isnull(df.Rating))
df_nan = df_nan[df_nan['Rating'] == True]
df_nan = df_nan.reset_index()

#start with initilising the movie_id variable as 1 adn keep on incrementing it tilll it reaches the end 
movie_np = []
movie_id = 1

for i,j in zip(df_nan['index'][1:],df_nan['index'][:-1]):
    temp = np.full((1,i-j-1), movie_id)
    movie_np = np.append(movie_np, temp)
    movie_id += 1
#need to handle the last record saperately as the last part filling the last few rows with the movie_id to match the size of the dataframe
last = np.full((1,len(df) - df_nan.iloc[-1, 0] - 1),movie_id)
movie_np = np.append(movie_np, last)

"""## **Slicing and cleaning the dataset Part 1**
1. Data is Huge with 96 Million ratings so its impossible to use for prediction, kernel runs out of memory.
2. Removing Movies that have few reviews(Non popular Movies)
3. Removing customers who had given less reviews(Less active Users)
4. Keeping recourses in mind taking only top 70% of movies and ratings
"""

f = ['count','mean']

df_movie_summary = df.groupby('Movie_Id')['Rating'].agg(f)
df_movie_summary.index = df_movie_summary.index.map(int)

movie_benchmark = round(df_movie_summary['count'].quantile(0.7),0)
cust_benchmark = round(df_cust_summary['count'].quantile(0.7),0)

drop_movie_list = df_movie_summary[df_movie_summary['count'] < movie_benchmark].index


df_cust_summary = df.groupby('Cust_Id')['Rating'].agg(f)
df_cust_summary.index = df_cust_summary.index.map(int)
drop_cust_list = df_cust_summary[df_cust_summary['count'] < cust_benchmark].index

print('Movie minimum times of review: {}'.format(movie_benchmark))
print('Customer minimum times of review: {}'.format(cust_benchmark))

"""## Final Dataset
1. The below is the original and final shape of the dataset after all the preprocessings are done and now this will be used for creating the final recommendation system.
2. containing Cust_id, Rating, Movie_id as columns
"""

print('Original Shape: {}'.format(df.shape))
# triming the dataset by removing the contetns in the swap list
df = df[~df['Movie_Id'].isin(drop_movie_list)] 
df = df[~df['Cust_Id'].isin(drop_cust_list)]
print('After Trim Shape: {}'.format(df.shape))
print(df.iloc[::100, :])

"""## **Creating adjacancy matrix**
* here we are creating a pivot table which will be having the all users and the movies they have rated so far.
* now our task is to predict the unkown ratings using the known ones even though the user havent seen it.
"""

df_p = pd.pivot_table(df,values='Rating',index='Cust_Id',columns='Movie_Id')
print(df_p.shape)

df_title = pd.read_csv('/content/drive/MyDrive/Data_Mining_Project/movie_titles.csv', encoding = "latin", header = None, names = ['Movie_Id', 'Year', 'Name'])
df_title.set_index('Movie_Id', inplace = True)
print (df_title.head())

"""To estimate all the unknown, we minimize the following regularized squared
error
The minimization is performed by a very straightforward stochastic gradient
descent
"""

reader = Reader()

data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']][:], reader)

#Building the model 
svd = SVD()
cross_validate(svd, data, measures=['RMSE', 'MAE'])

df_c = df[(df['Cust_Id'] == 7) & (df['Rating'] == 5)]
df_c = df_c.set_index('Movie_Id')
df_c = df_c.join(df_title)['Name']
print(df_c)

user_7 = df_title.copy()
user_7 = user_7.reset_index()
user_7 = user_7[~user_7['Movie_Id'].isin(drop_movie_list)]

# getting full dataset
data = Dataset.load_from_df(df[['Cust_Id', 'Movie_Id', 'Rating']], reader)

trainset = data.build_full_trainset()
svd.fit(trainset)

user_7['Estimate_Score'] = user_7['Movie_Id'].apply(lambda x: svd.predict(785314, x).est)

user_7 = user_7.drop('Movie_Id', axis = 1)

user_7 = user_7.sort_values('Estimate_Score', ascending=False)
print(user_7.head(10))